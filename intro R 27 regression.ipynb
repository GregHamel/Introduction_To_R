{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to R Part 27: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last few lessons we learned about statistical inference techniques including the t-test, chi-squared test and ANOVA which let you analyze differences bewteen data samples. Predictive modeling--using a data samples to make predicions about unseen data, such as data that has yet to be generated--is another common data analytics task. Predictive modeling is a form of machine learning, which describes using computers to automate the process of finding patterns in data.\n",
    "\n",
    "Machine learning is the driving force behind all kinds of modern conveniences and automation systems like ATMs that can read handwritten text, smartphones that translate speach to text and self-driving cars. The methods used in such cutting-edge applications are more advanced than anything we'll cover in this introduction, but they are all based on the principles of taking data and applying some learning algorithm to it to arrive at some sort of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a predictive modeling technique for predicting a numeric response variable based on one or more explanitory variables. The term \"regression\" in predictive modeling generally refers to any modeling task that invovles predicting a real number (as opposed classification, which invovles predicting a category or class.). The term \"linear\" in the name linear regression refers to the fact that the method models data with linear combination of the explanitory variables. A linear combination is an expression where one or more variables are scaled by a constant factor and added together. In the case of linear regression with a single explanitory varaible, the linear combination used in linear regression can be expressed as:\n",
    "\n",
    "$$ response = intercept + constant*explanitory$$\n",
    "\n",
    "The right side if the equation defines a line with a certain y-intercept and slope times the explanitory variable. In other words, linear regression in its most basic form fits a straight line to the response variable. The model is designed to fit a line that minimizes the squared differences (also called errors or residuals.). We won't go into all the math behind how the model actually minimizes the squared errors, but the end result is a line intended to give the \"best fit\" to the data. Since linear regression fits data with a line, it is most effective in cases where the response and explanitory variable have a linear relationship.\n",
    "\n",
    "Let's revisit the mtcars data set and use linear regression to predict vehicle gas mileage based on vehcile weight. First, lets look at a scatterplot of weight and mpg to get a sense of the shape of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myplot <- ggplot(data=mtcars, aes(x=wt, y=mpg)) +\n",
    "    geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows a roughly linear relationship bewteen weight and mgp, suggesting a linear regression model might work well.\n",
    "\n",
    "To fit a linear model in R, pass a formula specifing the model to the lm() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***\n",
       "wt           -5.3445     0.5591  -9.559 1.29e-10 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 3.046 on 30 degrees of freedom\n",
       "Multiple R-squared:  0.7528,\tAdjusted R-squared:  0.7446 \n",
       "F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_model <- lm(mpg ~ wt,       # formula of the form response ~ explanitory\n",
    "                data=mtcars)    # data set to use\n",
    "\n",
    "summary(mpg_model)              # view a summary of the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows the formula used to make te model, followed by a five number summary of the residuals and a summary of the model coefficients. The coefficients are the constants used to create the best fit line: in this case the y-intercept term is set to 37.2851 and the coefficient for the weight variable is -5.3445. In other words, the model fit the line mpg = 37.2851 -5.3445*wt. \n",
    "\n",
    "The t-value and Pr(>|t|) (p-value) columns indicate the results of conducting a t-test checking whether the coefficients chosen differ significantly from zero. In this case, there is near certainty that both the y-intercept and weight variable coefficients are not zero given the extremely low p-values, meaning both the intercept term and the weight are likely to be useful for predicting mpg. \n",
    "\n",
    "At the bottom of the summary output, the values of interest to us are \"Multiple R-squared\" and \"Adjusted R-squared.\" Multiple R-squared is a value that describes the proportion of the variance in the response variable explained by the model. In this case, it basically tells how much of the variation in mpg can be explained by weight. Adjusted R-squared is a modification of the normal R-squared that adjusts for models that invovle multiple explanitory variables. As you add more explanitory varaibles to a model, the normal R-squared reading can only increase, even if those variables add little informaiton to the model. In general we'd like to keep models as simple as possible, so adjusted R-squared is preferable for assessing the explanitory power of the model. In this case, roughly 75% of the variation in mpg is explained by weight.\n",
    "\n",
    "Let's plot the regression line on the scatterplot to get a sense of how well it fits the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myplot <- ggplot(data=mtcars, aes(x=wt, y=mpg)) +\n",
    "    geom_point() +\n",
    "    geom_abline(intercept = 37.2851, slope = -5.3445, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression line looks like a reasonable fit and it follows our intution: as car weight increases we would expect fuel economy to decline. \n",
    "\n",
    "Outliers can have a large influence on linear regression models: since regression deals with minimizing squared residuals, large residuals have a disproportinally large influence on the model. Plotting the result helps us detect influential outliers. In this case there does not appear to be any influential outliers. Let's add an outlier--a super heavy fuel efficent car--and plot a new regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "super_car <- c(50,10)                                  # outlier car \n",
    "new_cars <- rbind(mtcars[,c(\"mpg\",\"wt\")], super_car)   # add outlier to mtcars\n",
    "\n",
    "myplot <- ggplot(data=new_cars, aes(x=new_cars$wt, y=new_cars$mpg))+\n",
    "    geom_point() +\n",
    "    geom_smooth(method=lm,      # add a linear regression line\n",
    "                se=FALSE,       # don't add shaded confidence region\n",
    "                color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is an extreme, contrived case, the plot above illustrates how much influence a single outlier can have on a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a model, we'd like to be able to use it to make predictions. R contains a built-in function called predict() you can use to generate predictions based on a specified model for the original data set or for new data with the same variables as the original data. Let's use predict() to make predictions on the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Mazda RX4</dt>\n",
       "\t\t<dd>23.2826106468086</dd>\n",
       "\t<dt>Mazda RX4 Wag</dt>\n",
       "\t\t<dd>21.9197703957643</dd>\n",
       "\t<dt>Datsun 710</dt>\n",
       "\t\t<dd>24.8859521186254</dd>\n",
       "\t<dt>Hornet 4 Drive</dt>\n",
       "\t\t<dd>20.1026500610386</dd>\n",
       "\t<dt>Hornet Sportabout</dt>\n",
       "\t\t<dd>18.900143957176</dd>\n",
       "\t<dt>Valiant</dt>\n",
       "\t\t<dd>18.7932545257216</dd>\n",
       "\t<dt>Duster 360</dt>\n",
       "\t\t<dd>18.2053626527221</dd>\n",
       "\t<dt>Merc 240D</dt>\n",
       "\t\t<dd>20.2362618503567</dd>\n",
       "\t<dt>Merc 230</dt>\n",
       "\t\t<dd>20.4500407132656</dd>\n",
       "\t<dt>Merc 280</dt>\n",
       "\t\t<dd>18.900143957176</dd>\n",
       "\t<dt>Merc 280C</dt>\n",
       "\t\t<dd>18.900143957176</dd>\n",
       "\t<dt>Merc 450SE</dt>\n",
       "\t\t<dd>15.5331268663607</dd>\n",
       "\t<dt>Merc 450SL</dt>\n",
       "\t\t<dd>17.3502472010864</dd>\n",
       "\t<dt>Merc 450SLC</dt>\n",
       "\t\t<dd>17.0830236224503</dd>\n",
       "\t<dt>Cadillac Fleetwood</dt>\n",
       "\t\t<dd>9.22665041054798</dd>\n",
       "\t<dt>Lincoln Continental</dt>\n",
       "\t\t<dd>8.29671235689423</dd>\n",
       "\t<dt>Chrysler Imperial</dt>\n",
       "\t\t<dd>8.71892561113932</dd>\n",
       "\t<dt>Fiat 128</dt>\n",
       "\t\t<dd>25.5272887073521</dd>\n",
       "\t<dt>Honda Civic</dt>\n",
       "\t\t<dd>28.6538045773949</dd>\n",
       "\t<dt>Toyota Corolla</dt>\n",
       "\t\t<dd>27.4780208313959</dd>\n",
       "\t<dt>Toyota Corona</dt>\n",
       "\t\t<dd>24.1110037405806</dd>\n",
       "\t<dt>Dodge Challenger</dt>\n",
       "\t\t<dd>18.4725862313582</dd>\n",
       "\t<dt>AMC Javelin</dt>\n",
       "\t\t<dd>18.9268663150396</dd>\n",
       "\t<dt>Camaro Z28</dt>\n",
       "\t\t<dd>16.7623553280869</dd>\n",
       "\t<dt>Pontiac Firebird</dt>\n",
       "\t\t<dd>16.7356329702233</dd>\n",
       "\t<dt>Fiat X1-9</dt>\n",
       "\t\t<dd>26.9435736741236</dd>\n",
       "\t<dt>Porsche 914-2</dt>\n",
       "\t\t<dd>25.8479570017155</dd>\n",
       "\t<dt>Lotus Europa</dt>\n",
       "\t\t<dd>29.1989406778126</dd>\n",
       "\t<dt>Ford Pantera L</dt>\n",
       "\t\t<dd>20.3431512818111</dd>\n",
       "\t<dt>Ferrari Dino</dt>\n",
       "\t\t<dd>22.4809399109002</dd>\n",
       "\t<dt>Maserati Bora</dt>\n",
       "\t\t<dd>18.2053626527221</dd>\n",
       "\t<dt>Volvo 142E</dt>\n",
       "\t\t<dd>22.427495195173</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Mazda RX4] 23.2826106468086\n",
       "\\item[Mazda RX4 Wag] 21.9197703957643\n",
       "\\item[Datsun 710] 24.8859521186254\n",
       "\\item[Hornet 4 Drive] 20.1026500610386\n",
       "\\item[Hornet Sportabout] 18.900143957176\n",
       "\\item[Valiant] 18.7932545257216\n",
       "\\item[Duster 360] 18.2053626527221\n",
       "\\item[Merc 240D] 20.2362618503567\n",
       "\\item[Merc 230] 20.4500407132656\n",
       "\\item[Merc 280] 18.900143957176\n",
       "\\item[Merc 280C] 18.900143957176\n",
       "\\item[Merc 450SE] 15.5331268663607\n",
       "\\item[Merc 450SL] 17.3502472010864\n",
       "\\item[Merc 450SLC] 17.0830236224503\n",
       "\\item[Cadillac Fleetwood] 9.22665041054798\n",
       "\\item[Lincoln Continental] 8.29671235689423\n",
       "\\item[Chrysler Imperial] 8.71892561113932\n",
       "\\item[Fiat 128] 25.5272887073521\n",
       "\\item[Honda Civic] 28.6538045773949\n",
       "\\item[Toyota Corolla] 27.4780208313959\n",
       "\\item[Toyota Corona] 24.1110037405806\n",
       "\\item[Dodge Challenger] 18.4725862313582\n",
       "\\item[AMC Javelin] 18.9268663150396\n",
       "\\item[Camaro Z28] 16.7623553280869\n",
       "\\item[Pontiac Firebird] 16.7356329702233\n",
       "\\item[Fiat X1-9] 26.9435736741236\n",
       "\\item[Porsche 914-2] 25.8479570017155\n",
       "\\item[Lotus Europa] 29.1989406778126\n",
       "\\item[Ford Pantera L] 20.3431512818111\n",
       "\\item[Ferrari Dino] 22.4809399109002\n",
       "\\item[Maserati Bora] 18.2053626527221\n",
       "\\item[Volvo 142E] 22.427495195173\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Mazda RX4\n",
       ":   23.2826106468086Mazda RX4 Wag\n",
       ":   21.9197703957643Datsun 710\n",
       ":   24.8859521186254Hornet 4 Drive\n",
       ":   20.1026500610386Hornet Sportabout\n",
       ":   18.900143957176Valiant\n",
       ":   18.7932545257216Duster 360\n",
       ":   18.2053626527221Merc 240D\n",
       ":   20.2362618503567Merc 230\n",
       ":   20.4500407132656Merc 280\n",
       ":   18.900143957176Merc 280C\n",
       ":   18.900143957176Merc 450SE\n",
       ":   15.5331268663607Merc 450SL\n",
       ":   17.3502472010864Merc 450SLC\n",
       ":   17.0830236224503Cadillac Fleetwood\n",
       ":   9.22665041054798Lincoln Continental\n",
       ":   8.29671235689423Chrysler Imperial\n",
       ":   8.71892561113932Fiat 128\n",
       ":   25.5272887073521Honda Civic\n",
       ":   28.6538045773949Toyota Corolla\n",
       ":   27.4780208313959Toyota Corona\n",
       ":   24.1110037405806Dodge Challenger\n",
       ":   18.4725862313582AMC Javelin\n",
       ":   18.9268663150396Camaro Z28\n",
       ":   16.7623553280869Pontiac Firebird\n",
       ":   16.7356329702233Fiat X1-9\n",
       ":   26.9435736741236Porsche 914-2\n",
       ":   25.8479570017155Lotus Europa\n",
       ":   29.1989406778126Ford Pantera L\n",
       ":   20.3431512818111Ferrari Dino\n",
       ":   22.4809399109002Maserati Bora\n",
       ":   18.2053626527221Volvo 142E\n",
       ":   22.427495195173\n",
       "\n"
      ],
      "text/plain": [
       "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
       "          23.282611           21.919770           24.885952           20.102650 \n",
       "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
       "          18.900144           18.793255           18.205363           20.236262 \n",
       "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
       "          20.450041           18.900144           18.900144           15.533127 \n",
       "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
       "          17.350247           17.083024            9.226650            8.296712 \n",
       "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
       "           8.718926           25.527289           28.653805           27.478021 \n",
       "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
       "          24.111004           18.472586           18.926866           16.762355 \n",
       "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
       "          16.735633           26.943574           25.847957           29.198941 \n",
       "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
       "          20.343151           22.480940           18.205363           22.427495 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds <- predict(mpg_model,    # model to use for prediction\n",
    "         newdata=mtcars)       # data to use for prediction\n",
    "\n",
    "preds                          # check predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: In practice, we'd typically want to make predictions based on some new unseen data (test data) that did not play a part in creating the original model. We will see examples of making predictions on test data when we revisit the Titinic survival prediction competition in coming lessons.\n",
    "\n",
    "We can check the differences bewteen our predictions and the actual values by subtracting the predictions from the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>Mazda RX4</dt>\n",
       "\t\t<dd>-2.28261064680861</dd>\n",
       "\t<dt>Mazda RX4 Wag</dt>\n",
       "\t\t<dd>-0.919770395764324</dd>\n",
       "\t<dt>Datsun 710</dt>\n",
       "\t\t<dd>-2.08595211862541</dd>\n",
       "\t<dt>Hornet 4 Drive</dt>\n",
       "\t\t<dd>1.29734993896138</dd>\n",
       "\t<dt>Hornet Sportabout</dt>\n",
       "\t\t<dd>-0.200143957176017</dd>\n",
       "\t<dt>Valiant</dt>\n",
       "\t\t<dd>-0.69325452572156</dd>\n",
       "\t<dt>Duster 360</dt>\n",
       "\t\t<dd>-3.90536265272207</dd>\n",
       "\t<dt>Merc 240D</dt>\n",
       "\t\t<dd>4.16373814964332</dd>\n",
       "\t<dt>Merc 230</dt>\n",
       "\t\t<dd>2.34995928673441</dd>\n",
       "\t<dt>Merc 280</dt>\n",
       "\t\t<dd>0.299856042823983</dd>\n",
       "\t<dt>Merc 280C</dt>\n",
       "\t\t<dd>-1.10014395717602</dd>\n",
       "\t<dt>Merc 450SE</dt>\n",
       "\t\t<dd>0.86687313363927</dd>\n",
       "\t<dt>Merc 450SL</dt>\n",
       "\t\t<dd>-0.0502472010864388</dd>\n",
       "\t<dt>Merc 450SLC</dt>\n",
       "\t\t<dd>-1.88302362245031</dd>\n",
       "\t<dt>Cadillac Fleetwood</dt>\n",
       "\t\t<dd>1.17334958945202</dd>\n",
       "\t<dt>Lincoln Continental</dt>\n",
       "\t\t<dd>2.10328764310577</dd>\n",
       "\t<dt>Chrysler Imperial</dt>\n",
       "\t\t<dd>5.98107438886068</dd>\n",
       "\t<dt>Fiat 128</dt>\n",
       "\t\t<dd>6.87271129264787</dd>\n",
       "\t<dt>Honda Civic</dt>\n",
       "\t\t<dd>1.7461954226051</dd>\n",
       "\t<dt>Toyota Corolla</dt>\n",
       "\t\t<dd>6.42197916860409</dd>\n",
       "\t<dt>Toyota Corona</dt>\n",
       "\t\t<dd>-2.61100374058062</dd>\n",
       "\t<dt>Dodge Challenger</dt>\n",
       "\t\t<dd>-2.9725862313582</dd>\n",
       "\t<dt>AMC Javelin</dt>\n",
       "\t\t<dd>-3.72686631503963</dd>\n",
       "\t<dt>Camaro Z28</dt>\n",
       "\t\t<dd>-3.46235532808695</dd>\n",
       "\t<dt>Pontiac Firebird</dt>\n",
       "\t\t<dd>2.46436702977667</dd>\n",
       "\t<dt>Fiat X1-9</dt>\n",
       "\t\t<dd>0.35642632587636</dd>\n",
       "\t<dt>Porsche 914-2</dt>\n",
       "\t\t<dd>0.152042998284507</dd>\n",
       "\t<dt>Lotus Europa</dt>\n",
       "\t\t<dd>1.20105932218739</dd>\n",
       "\t<dt>Ford Pantera L</dt>\n",
       "\t\t<dd>-4.54315128181114</dd>\n",
       "\t<dt>Ferrari Dino</dt>\n",
       "\t\t<dd>-2.78093991090021</dd>\n",
       "\t<dt>Maserati Bora</dt>\n",
       "\t\t<dd>-3.20536265272207</dd>\n",
       "\t<dt>Volvo 142E</dt>\n",
       "\t\t<dd>-1.02749519517298</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[Mazda RX4] -2.28261064680861\n",
       "\\item[Mazda RX4 Wag] -0.919770395764324\n",
       "\\item[Datsun 710] -2.08595211862541\n",
       "\\item[Hornet 4 Drive] 1.29734993896138\n",
       "\\item[Hornet Sportabout] -0.200143957176017\n",
       "\\item[Valiant] -0.69325452572156\n",
       "\\item[Duster 360] -3.90536265272207\n",
       "\\item[Merc 240D] 4.16373814964332\n",
       "\\item[Merc 230] 2.34995928673441\n",
       "\\item[Merc 280] 0.299856042823983\n",
       "\\item[Merc 280C] -1.10014395717602\n",
       "\\item[Merc 450SE] 0.86687313363927\n",
       "\\item[Merc 450SL] -0.0502472010864388\n",
       "\\item[Merc 450SLC] -1.88302362245031\n",
       "\\item[Cadillac Fleetwood] 1.17334958945202\n",
       "\\item[Lincoln Continental] 2.10328764310577\n",
       "\\item[Chrysler Imperial] 5.98107438886068\n",
       "\\item[Fiat 128] 6.87271129264787\n",
       "\\item[Honda Civic] 1.7461954226051\n",
       "\\item[Toyota Corolla] 6.42197916860409\n",
       "\\item[Toyota Corona] -2.61100374058062\n",
       "\\item[Dodge Challenger] -2.9725862313582\n",
       "\\item[AMC Javelin] -3.72686631503963\n",
       "\\item[Camaro Z28] -3.46235532808695\n",
       "\\item[Pontiac Firebird] 2.46436702977667\n",
       "\\item[Fiat X1-9] 0.35642632587636\n",
       "\\item[Porsche 914-2] 0.152042998284507\n",
       "\\item[Lotus Europa] 1.20105932218739\n",
       "\\item[Ford Pantera L] -4.54315128181114\n",
       "\\item[Ferrari Dino] -2.78093991090021\n",
       "\\item[Maserati Bora] -3.20536265272207\n",
       "\\item[Volvo 142E] -1.02749519517298\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "Mazda RX4\n",
       ":   -2.28261064680861Mazda RX4 Wag\n",
       ":   -0.919770395764324Datsun 710\n",
       ":   -2.08595211862541Hornet 4 Drive\n",
       ":   1.29734993896138Hornet Sportabout\n",
       ":   -0.200143957176017Valiant\n",
       ":   -0.69325452572156Duster 360\n",
       ":   -3.90536265272207Merc 240D\n",
       ":   4.16373814964332Merc 230\n",
       ":   2.34995928673441Merc 280\n",
       ":   0.299856042823983Merc 280C\n",
       ":   -1.10014395717602Merc 450SE\n",
       ":   0.86687313363927Merc 450SL\n",
       ":   -0.0502472010864388Merc 450SLC\n",
       ":   -1.88302362245031Cadillac Fleetwood\n",
       ":   1.17334958945202Lincoln Continental\n",
       ":   2.10328764310577Chrysler Imperial\n",
       ":   5.98107438886068Fiat 128\n",
       ":   6.87271129264787Honda Civic\n",
       ":   1.7461954226051Toyota Corolla\n",
       ":   6.42197916860409Toyota Corona\n",
       ":   -2.61100374058062Dodge Challenger\n",
       ":   -2.9725862313582AMC Javelin\n",
       ":   -3.72686631503963Camaro Z28\n",
       ":   -3.46235532808695Pontiac Firebird\n",
       ":   2.46436702977667Fiat X1-9\n",
       ":   0.35642632587636Porsche 914-2\n",
       ":   0.152042998284507Lotus Europa\n",
       ":   1.20105932218739Ford Pantera L\n",
       ":   -4.54315128181114Ferrari Dino\n",
       ":   -2.78093991090021Maserati Bora\n",
       ":   -3.20536265272207Volvo 142E\n",
       ":   -1.02749519517298\n",
       "\n"
      ],
      "text/plain": [
       "          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n",
       "         -2.2826106          -0.9197704          -2.0859521           1.2973499 \n",
       "  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n",
       "         -0.2001440          -0.6932545          -3.9053627           4.1637381 \n",
       "           Merc 230            Merc 280           Merc 280C          Merc 450SE \n",
       "          2.3499593           0.2998560          -1.1001440           0.8668731 \n",
       "         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n",
       "         -0.0502472          -1.8830236           1.1733496           2.1032876 \n",
       "  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n",
       "          5.9810744           6.8727113           1.7461954           6.4219792 \n",
       "      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n",
       "         -2.6110037          -2.9725862          -3.7268663          -3.4623553 \n",
       "   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n",
       "          2.4643670           0.3564263           0.1520430           1.2010593 \n",
       "     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n",
       "         -4.5431513          -2.7809399          -3.2053627          -1.0274952 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals <- mtcars$mpg-preds      # check residuals\n",
    "residuals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a well-behaved linear regression model, we'd like the residuals to be roughly normally distributed. That is, we'd like a roughly even spread of error above and below the regression line. We can investigate the normality of residuals with a Q-Q (quantile-quantile) plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# qqnorm(residuals)       # make a QQ plot of residuals\n",
    "# qqline(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When residauls are normally distributed, they tend to lie along the stright line on the Q-Q plot. In this case residuals appear to follow a slighly non-linear pattern: the residuals are bowed a bit away from the normality line on each end. This is an indication that simple stright line might not be sufficient to fully describe the relationship bewteen weight and mpg.\n",
    "\n",
    "After making model predictions, it is useful to have some sort of metric to evaluate oh well the model performed. Adjusted R-squared is one useful measure, but it only applies to the regression model itself: we'd like some universal evaluation metric that lets us compare the performance of different types of models. Root mean squared error (RMSE) is a common evalaution metric for predictions invovling real numbers. Root mean squared error is square root of the average of the squared error (the squared residuals.). If you recall, we wrote a function to calcualte RMSE in lesson 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.94916268595503"
      ],
      "text/latex": [
       "2.94916268595503"
      ],
      "text/markdown": [
       "2.94916268595503"
      ],
      "text/plain": [
       "[1] 2.949163"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_mean_squared_error <- function(predicted, targets){  \n",
    "    # Computes root mean squared error between two vectors\n",
    "    #\n",
    "    # Args:\n",
    "    #    predicted: a numeric vector of predictions\n",
    "    #    targets: a numeric vector of target values for each prediction\n",
    "    #\n",
    "    # Returns:\n",
    "    #    The root mean squared error between predicted values and targets\n",
    "    \n",
    "    sqrt(mean((targets-predicted)^2))                 \n",
    "}\n",
    "\n",
    "root_mean_squared_error(preds, mtcars$mpg)           # compute RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use a pre-made RMSE function, such as the one provided in the caret package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n"
     ]
    }
   ],
   "source": [
    "library(caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.94916268595503"
      ],
      "text/latex": [
       "2.94916268595503"
      ],
      "text/markdown": [
       "2.94916268595503"
      ],
      "text/plain": [
       "[1] 2.949163"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(preds, mtcars$mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables often exhibit non-linear relationships that can't be fit well with a stright line. In these cases, we can use linear regression to fit a curved line the data by adding extra higher order terms (squared, cubic, etc.) to the model formula. A linear regression that invovles higher order terms is known as \"polynomial regression.\" You can include higher order terms by adding extra explanitory terms wrapped in the I() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + I(wt^2), data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "   Min     1Q Median     3Q    Max \n",
       "-3.483 -1.998 -0.773  1.462  6.238 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  49.9308     4.2113  11.856 1.21e-12 ***\n",
       "wt          -13.3803     2.5140  -5.322 1.04e-05 ***\n",
       "I(wt^2)       1.1711     0.3594   3.258  0.00286 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.651 on 29 degrees of freedom\n",
       "Multiple R-squared:  0.8191,\tAdjusted R-squared:  0.8066 \n",
       "F-statistic: 65.64 on 2 and 29 DF,  p-value: 1.715e-11\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic_model <- lm(mpg ~ wt + I(wt^2),  # add the weight squared term to the model\n",
    "                      data=mtcars) \n",
    "\n",
    "summary(quadratic_model)                   # check the model result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary output shows us that including the weight squared term appears to improve the model's performance because the adjusted R-squared increased to 0.8066 and the 2nd order polynomial term has a p-value of 0.00286, indicated that it is highly significant. Let's plot the curved line defined by the new model to see if the fit looks better than the old one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_function <- function(x){              # create a function based on the model\n",
    "    49.9308 + -13.3803*x + 1.1711*x^2\n",
    "}\n",
    "\n",
    "myplot <- ggplot(data=mtcars, aes(x=wt, y=mpg)) +\n",
    "    geom_point() +\n",
    "    stat_function(fun=model_function, color=\"red\")    # plot the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic function seems to fit the data a little better than the linear one. Let's investigate further by using the new model to make predictions and check the root mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.52330047246108"
      ],
      "text/latex": [
       "2.52330047246108"
      ],
      "text/markdown": [
       "2.52330047246108"
      ],
      "text/plain": [
       "[1] 2.5233"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds <- predict(quadratic_model,    # model to use for prediction\n",
    "         newdata=mtcars)             # data to use for prediction\n",
    "\n",
    "root_mean_squared_error(preds, mtcars$mpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RMSE of the qudratic model is lower than the old one and the adjusted R-squared is higher, it is probably a better model. We do, however, have to be careful about overfitting the data. \n",
    "\n",
    "Overfitting describes a situation where our model fits the data we use to create it (training data) too closely, resulting in poor generalization to new data. This is why we generally don't want to use training data to evaluate a model: it gives us an biased, usually overly optimisitc evaluation. One of the strengths of first and second order linear regression is that they are so simple, they are unlikley to overfit data very much. The more complex the model we create and the more freedom it has to fit the training data and the greater risk we run of overfitting. For example, we could keep including more polynomial terms in our regression model to fit the training data more closely and achieve lower RMSE scores against the training set, but this would almost certianly not generalize well to new data. Let's illustrate this point by fitting a 10th order model to the mtcars data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>-14921.1256679509</dd>\n",
       "\t<dt>wt</dt>\n",
       "\t\t<dd>64581.3723400433</dd>\n",
       "\t<dt>I(wt^2)</dt>\n",
       "\t\t<dd>-120086.155632163</dd>\n",
       "\t<dt>I(wt^3)</dt>\n",
       "\t\t<dd>126931.950142918</dd>\n",
       "\t<dt>I(wt^4)</dt>\n",
       "\t\t<dd>-84659.8581288786</dd>\n",
       "\t<dt>I(wt^5)</dt>\n",
       "\t\t<dd>37315.5247656017</dd>\n",
       "\t<dt>I(wt^6)</dt>\n",
       "\t\t<dd>-11033.4768062176</dd>\n",
       "\t<dt>I(wt^7)</dt>\n",
       "\t\t<dd>2165.90426790932</dd>\n",
       "\t<dt>I(wt^8)</dt>\n",
       "\t\t<dd>-270.730570029588</dd>\n",
       "\t<dt>I(wt^9)</dt>\n",
       "\t\t<dd>19.4974178769716</dd>\n",
       "\t<dt>I(wt^10)</dt>\n",
       "\t\t<dd>-0.615515485385577</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] -14921.1256679509\n",
       "\\item[wt] 64581.3723400433\n",
       "\\item[I(wt^2)] -120086.155632163\n",
       "\\item[I(wt^3)] 126931.950142918\n",
       "\\item[I(wt^4)] -84659.8581288786\n",
       "\\item[I(wt^5)] 37315.5247656017\n",
       "\\item[I(wt^6)] -11033.4768062176\n",
       "\\item[I(wt^7)] 2165.90426790932\n",
       "\\item[I(wt^8)] -270.730570029588\n",
       "\\item[I(wt^9)] 19.4974178769716\n",
       "\\item[I(wt^10)] -0.615515485385577\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   -14921.1256679509wt\n",
       ":   64581.3723400433I(wt^2)\n",
       ":   -120086.155632163I(wt^3)\n",
       ":   126931.950142918I(wt^4)\n",
       ":   -84659.8581288786I(wt^5)\n",
       ":   37315.5247656017I(wt^6)\n",
       ":   -11033.4768062176I(wt^7)\n",
       ":   2165.90426790932I(wt^8)\n",
       ":   -270.730570029588I(wt^9)\n",
       ":   19.4974178769716I(wt^10)\n",
       ":   -0.615515485385577\n",
       "\n"
      ],
      "text/plain": [
       "  (Intercept)            wt       I(wt^2)       I(wt^3)       I(wt^4) \n",
       "-1.492113e+04  6.458137e+04 -1.200862e+05  1.269320e+05 -8.465986e+04 \n",
       "      I(wt^5)       I(wt^6)       I(wt^7)       I(wt^8)       I(wt^9) \n",
       " 3.731552e+04 -1.103348e+04  2.165904e+03 -2.707306e+02  1.949742e+01 \n",
       "     I(wt^10) \n",
       "-6.155155e-01 "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenth_order_model <- lm(mpg ~ wt + I(wt^2) + I(wt^3) + I(wt^4) + I(wt^5) +  # 10th order model\n",
    "                        I(wt^6) + I(wt^7) + I(wt^8) + I(wt^9) + +I(wt^10),\n",
    "                        data=mtcars)\n",
    "\n",
    "tenth_order_model$coefficients                  # check model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_function2 <- function(x){      # create a function based on the model coefficients\n",
    "    \n",
    "    -14921.1256679509 + 64581.3723400433*x + -120086.155632163*x^2 + 126931.950142918*x^3 +\n",
    "    -84659.8581288786*x^4 + 37315.5247656017*x^5 + -11033.4768062176*x^6 + 2165.90426790932*x^7 +\n",
    "    -270.730570029588*x^8 + 19.4974178769716*x^9 + -0.615515485385577*x^10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myplot <- ggplot(data=mtcars, aes(x=wt, y=mpg)) +               # plot the function\n",
    "    geom_point() +\n",
    "    stat_function(fun=model_function2, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the 10th order polynomial model curves wildly in some places to fit the training data. While this model happens to yield a closer fit to the training data, it is almost certainly will not to generalize well to new data as it leads to absurd predictions such as a car having less than 0 mpg if it weighs 5000lbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with a predictive modeling task, you'll often have several variables in your data that may help explain variation in the response variable. You can include more explanitory variables in a linear regression model by adding extra terms to the formula. Let's make a new model that adds the horsepower variable to our original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + hp, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "   Min     1Q Median     3Q    Max \n",
       "-3.941 -1.600 -0.182  1.050  5.854 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***\n",
       "wt          -3.87783    0.63273  -6.129 1.12e-06 ***\n",
       "hp          -0.03177    0.00903  -3.519  0.00145 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.593 on 29 degrees of freedom\n",
       "Multiple R-squared:  0.8268,\tAdjusted R-squared:  0.8148 \n",
       "F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_model <- lm(mpg ~ wt + hp,  # add horsepower\n",
    "                data=mtcars)    \n",
    "\n",
    "summary(mpg_model)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression output shows high signifiance for the horsepower variable and a better adusted R-squared score than any of our previous models. This improvement suggests horsepower has a linear relationship with mpg. Let's investigate with a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myplot <- ggplot(data=mtcars, aes(x=hp, y=mpg)) +               # plot the function\n",
    "    geom_point() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While mpg does tend to decline with horsepower, the relationship appears more curved than linear so adding polynomial terms to our multiple regression model could yield a better fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + hp + I(wt^2) + I(hp^2), data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-2.8849 -1.8165 -0.3922  1.3499  4.5807 \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  4.945e+01  3.521e+00  14.044 6.27e-14 ***\n",
       "wt          -9.220e+00  2.270e+00  -4.062 0.000375 ***\n",
       "hp          -9.428e-02  3.193e-02  -2.952 0.006456 ** \n",
       "I(wt^2)      8.500e-01  3.005e-01   2.829 0.008700 ** \n",
       "I(hp^2)      1.743e-04  8.073e-05   2.159 0.039879 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.135 on 27 degrees of freedom\n",
       "Multiple R-squared:  0.8907,\tAdjusted R-squared:  0.8745 \n",
       "F-statistic: 55.02 on 4 and 27 DF,  p-value: 1.363e-12\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "1.9609108134177"
      ],
      "text/latex": [
       "1.9609108134177"
      ],
      "text/markdown": [
       "1.9609108134177"
      ],
      "text/plain": [
       "[1] 1.960911"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_model <- lm(mpg ~ wt + hp + I(wt^2) + I(hp^2), # include squared terms\n",
    "                data=mtcars)\n",
    "\n",
    "summary(mpg_model)                    # view a summary of the regression model\n",
    "\n",
    "sqrt(mean((mpg_model$residuals)^2))   # calculate RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression output shows that both squared terms are statistically significant at the 95% confidence level and the higher adjusted R-squared and lower RMSE suggest this is a better model than any we made previously. Note that when working with multidimensional models, it becomes difficult to visualize results, so you rely heavily on numeric output.\n",
    "\n",
    "We could continue adding more explanitory variables in an attmept to improve the model. Sometimes you may wish to include every variable at your disposal into a model. In an R formula, you can use the period character \".\" as shorthand for \"include all variables\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ ., data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-3.4506 -1.6044 -0.1196  1.2193  4.6271 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)  \n",
       "(Intercept) 12.30337   18.71788   0.657   0.5181  \n",
       "cyl         -0.11144    1.04502  -0.107   0.9161  \n",
       "disp         0.01334    0.01786   0.747   0.4635  \n",
       "hp          -0.02148    0.02177  -0.987   0.3350  \n",
       "drat         0.78711    1.63537   0.481   0.6353  \n",
       "wt          -3.71530    1.89441  -1.961   0.0633 .\n",
       "qsec         0.82104    0.73084   1.123   0.2739  \n",
       "vs           0.31776    2.10451   0.151   0.8814  \n",
       "am           2.52023    2.05665   1.225   0.2340  \n",
       "gear         0.65541    1.49326   0.439   0.6652  \n",
       "carb        -0.19942    0.82875  -0.241   0.8122  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.65 on 21 degrees of freedom\n",
       "Multiple R-squared:  0.869,\tAdjusted R-squared:  0.8066 \n",
       "F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg_model <- lm(mpg ~ .,        # make a model with all mtcars variables\n",
    "                data=mtcars)\n",
    "\n",
    "summary(mpg_model)               # view a summary of the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears a model involving all variables does not perform as well as our weight and horsepower based model. Adding variables that have little relationship with the response or including variables that are too closely related to one another can hurt your results when using linear regression. You should also be wary of numeric variables that take on few unique values since they often act more like categorical variables than numeric ones. It may make improve your results to convert such variables into factors (The lm() function accepts factor variables.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is one of the most common techniques for making real numbered predictions from data. It is a good place to start any time you need to make a numeric prediction. Next time we'll revisit the titanic survival data set and focus classification: assigning observations to categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Time: Introduction to R Part 28: Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
